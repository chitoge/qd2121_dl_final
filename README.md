# qd2121_dl_final

**Team:** Spline Reticulator

**Author:** Thanh Do (qd2121) - only team member

## Summary

Repository for the Deep Learning final project (ECE-GY 7123 / CS-GY 6953, Fall '25). This repo contains the materials for a project on **Fine-Tuning Small Language Models for Targeted XML Fuzzing**. The objective is to use a fine-tuned Small Language Model (SLM) as a learned fuzzer to generate XML/XSD inputs that maximize code coverage in the `python-xmlschema` library. The approach leverages Low-Rank Adaptation (LoRA) to fine-tune `Qwen3-0.6B` on the W3C XML Schema Test Suite, exploiting the model's "hallucination" behaviors as semantic mutations for effective fuzzing.

## Key results

- Successfully fine-tuned `Qwen3-0.6B` with LoRA on W3C XML Schema Test Suite
- Generated XML/XSD samples show higher code coverage compared to zero-shot baselines
- Demonstrated that SLM "hallucinations" may serve as effective semantic mutations for structural fuzzing
- Model achieves lower perplexity on valid XML/XSD data after fine-tuning

## Repository contents

- `report/` — LaTeX source of the project report. Main file: `report/acl_latex.tex`.
- `training_notebook.ipynb` — training notebook used for fine-tuning the model with LoRA and generating test samples.
- `baseline_samples/` — XML/XSD samples generated by the zero-shot baseline model (100 files).
- `lora_samples/` — XML/XSD samples generated by the LoRA-tuned model (100 files).
- `DL_Finals.pdf` — prebuilt PDF report submitted to Gradescope.

## High-level method

- **Dataset:** W3C XML Schema Test Suite (xsdtests). Contains valid and invalid XML/XSD files for training.
- **Base model:** `Qwen/Qwen3-0.6B` (a Small Language Model pre-trained on code).
- **Training:** LoRA fine-tuning with `r=8`, `lora_alpha=16`, trained with `Trainer` (Hugging Face Transformers) using `adamw_torch`, sequence length 1024, 15 epochs.
- **Target Library:** `python-xmlschema` version 4.2.0 — used for coverage evaluation.
- **Approach:** No-prompt training strategy (raw code, no instruction prompts) to bypass alignment and maximize generation of edge cases.

## How to run the notebooks (local / GPU)

### Requirements

- Python 3.10+ (recommended)
- CUDA-capable GPU
- The notebooks include dependency/setup cells (install Hugging Face libraries, PyTorch, coverage, xmlschema, etc.). Run those first.

### Basic steps

1. Open `training_notebook.ipynb` in Google Colab or locally with GPU support.
2. Run the dependency/setup cells to install required packages (`transformers`, `peft`, `datasets`, `torch`, `coverage`, `xmlschema`).
3. Mount Google Drive (if using Colab) and configure paths to the W3C dataset.
4. Run training cells to fine-tune the model with LoRA.
5. Run generation cells to produce XML/XSD samples and evaluate code coverage on `python-xmlschema`.

## CI: building the PDF report

A GitHub Actions workflow is provided at `.github/workflows/build-report.yml`. You can get the latest report by fetching the latest `report-pdf` artifact from the run page.

A prebuilt report file is included in the repo as `DL_Finals.pdf` which was submitted to Gradescope.

Please note that in order to view the Colab notebooks and access any shared resources, you'll need to log in to Google with your NYU account.
